# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XoX1Li3H5sSgFtU_YaPYKJHE1O6_hLAk
"""

# Single-cell: ClauseWise (fixed) ‚Äî Colab-ready
# - Uses ibm-granite/granite-3.3-2b-instruct with trust_remote_code=True
# - Robust file reading (avoids NamedString.read errors)
# - Gradio UI for upload / paste + tasks
# NOTE: Enable GPU in Colab runtime (Runtime > Change runtime type > GPU) for faster local inference.

# Install dependencies (already satisfied in your session but safe to keep)
!pip install -q transformers accelerate huggingface-hub pdfplumber python-docx gradio

# ---------- Imports ----------
import os, io, re, json, textwrap, tempfile
from pathlib import Path
from typing import List
import gradio as gr
import pdfplumber
import docx
import torch
from huggingface_hub import login as hf_login, whoami
from transformers import AutoTokenizer, AutoModelForCausalLM

# -----------------------------
# Config
# -----------------------------
MODEL_NAME = "ibm-granite/granite-3.3-2b-instruct"
MAX_NEW_TOKENS = 512

# -----------------------------
# Robust file reading helpers (fixes NamedString errors)
# -----------------------------
def _read_bytes_from_maybe_fileobj(obj) -> bytes:
    # Already bytes
    if isinstance(obj, (bytes, bytearray)):
        return bytes(obj)
    # If it's a plain path string
    if isinstance(obj, str) and os.path.exists(obj):
        with open(obj, "rb") as f:
            return f.read()
    # Dict-like wrappers from some runtimes
    if isinstance(obj, dict):
        if "data" in obj and isinstance(obj["data"], (bytes, bytearray)):
            return bytes(obj["data"])
        if "tmp_path" in obj and os.path.exists(obj["tmp_path"]):
            with open(obj["tmp_path"], "rb") as f:
                return f.read()
        if "name" in obj and "file" in obj and hasattr(obj["file"], "read"):
            return obj["file"].read()
    # If object has .read()
    if hasattr(obj, "read") and callable(obj.read):
        data = obj.read()
        if isinstance(data, str):
            return data.encode("utf-8")
        return data
    # If object has .name path attribute
    if hasattr(obj, "name") and isinstance(obj.name, str) and os.path.exists(obj.name):
        with open(obj.name, "rb") as f:
            return f.read()
    # Maybe a tuple (name, path) or (path,)
    if isinstance(obj, (list, tuple)):
        for item in obj:
            if isinstance(item, str) and os.path.exists(item):
                with open(item, "rb") as f:
                    return f.read()
    raise ValueError("Unsupported uploaded_file type ‚Äî could not read bytes. Got type: " + str(type(obj)))

def extract_text_from_pdf_bytes(b: bytes) -> str:
    out = []
    with pdfplumber.open(io.BytesIO(b)) as pdf:
        for p in pdf.pages:
            t = p.extract_text()
            if t:
                out.append(t)
    return "\n\n".join(out).strip()

def extract_text_from_docx_bytes(b: bytes) -> str:
    # python-docx wants a file path; write to temp
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
        tmp.write(b)
        tmp_path = tmp.name
    try:
        doc = docx.Document(tmp_path)
        paras = [p.text for p in doc.paragraphs if p.text and p.text.strip()]
        return "\n\n".join(paras).strip()
    finally:
        try:
            os.remove(tmp_path)
        except Exception:
            pass

def extract_text_from_txt_bytes(b: bytes) -> str:
    try:
        return b.decode("utf-8")
    except Exception:
        return b.decode("latin-1", errors="ignore")

def extract_text(uploaded_file) -> str:
    if uploaded_file is None:
        return ""
    raw_bytes = _read_bytes_from_maybe_fileobj(uploaded_file)
    # try find filename if present
    file_name = None
    if isinstance(uploaded_file, dict) and "name" in uploaded_file:
        file_name = uploaded_file["name"].lower()
    elif hasattr(uploaded_file, "name"):
        try:
            file_name = uploaded_file.name.lower()
        except Exception:
            file_name = None
    # by extension
    if file_name and file_name.endswith(".pdf"):
        return extract_text_from_pdf_bytes(raw_bytes)
    if file_name and (file_name.endswith(".docx") or file_name.endswith(".doc")):
        return extract_text_from_docx_bytes(raw_bytes)
    if file_name and file_name.endswith(".txt"):
        return extract_text_from_txt_bytes(raw_bytes)
    # magic bytes
    header = raw_bytes[:8]
    if header.startswith(b"%PDF"):
        return extract_text_from_pdf_bytes(raw_bytes)
    if header.startswith(b"PK\x03\x04"):  # likely docx
        return extract_text_from_docx_bytes(raw_bytes)
    # fallback -> treat as text
    return extract_text_from_txt_bytes(raw_bytes)

# -----------------------------
# Clause splitting helper
# -----------------------------
def split_clauses(text: str, min_len=40) -> List[str]:
    if not text:
        return []
    parts = re.split(r'\n{2,}|\r\n{2,}', text)
    clauses = []
    for p in parts:
        sub = re.split(r'(?m)^\s*\d+\.\s+|\n-\s+|;\s+', p)
        for s in sub:
            s2 = s.strip()
            if len(s2) >= min_len:
                clauses.append(s2)
    if not clauses:
        sentences = re.split(r'(?<=[.!?])\s+', text)
        clauses = [s.strip() for s in sentences if len(s.strip()) >= min_len]
    seen = set()
    out = []
    for c in clauses:
        if c not in seen:
            seen.add(c)
            out.append(c)
    return out

# -----------------------------
# Model loader (local) ‚Äî correct loader with trust_remote_code
# -----------------------------
_tokenizer = None
_model = None
_model_device = "cpu"

def load_model(hf_token: str = None):
    global _tokenizer, _model, _model_device
    if _model is not None and _tokenizer is not None:
        return _tokenizer, _model
    # If user provided token, try to login (so private models work)
    if hf_token:
        try:
            hf_login(hf_token)
        except Exception as e:
            print("HF login failed:", e)
    # Determine device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    _model_device = device
    try:
        print(f"Loading tokenizer and model {MODEL_NAME} with trust_remote_code=True on {device} ...")
        _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
        _model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True,
            device_map="auto" if device == "cuda" else None,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            low_cpu_mem_usage=True
        )
        print("Model loaded.")
    except Exception as e:
        # Provide helpful message
        raise RuntimeError(
            "Failed to load the model locally. This can happen if the repository requires authentication or the "
            "custom code can't be loaded. Error: " + str(e)
        )
    return _tokenizer, _model

# -----------------------------
# Safe generation wrapper
# -----------------------------
def generate_with_model(prompt: str, hf_token: str = None, max_new_tokens: int = MAX_NEW_TOKENS) -> str:
    tok, mdl = load_model(hf_token)
    device = _model_device
    # Prepare input
    inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=2048).to(mdl.device)
    # Generate
    gen_ids = mdl.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, eos_token_id=tok.eos_token_id)
    out = tok.decode(gen_ids[0], skip_special_tokens=True)
    # If model returns prompt+generation, try to strip prompt prefix
    if out.startswith(prompt):
        return out[len(prompt):].strip()
    return out

# -----------------------------
# Prompt builders for tasks
# -----------------------------
def build_prompt_simplify(text: str) -> str:
    return textwrap.dedent(f"""Simplify the following legal clause or document into clear plain English. Keep legal meaning intact. Output short (3-6 sentences) starting with "Simplified:"\n\n{text}""")

def build_prompt_ner(text: str) -> str:
    return textwrap.dedent(f"""Extract important legal entities from the text and output JSON array items with keys: type (PARTY/DATE/AMOUNT/OBLIGATION/TERM/GOVERNING_LAW/OTHER), text, span_start, span_end.\n\nText:\n{text}""")

def build_prompt_clause_breakdown(text: str) -> str:
    return textwrap.dedent(f"""Break the document into individual clauses; for each clause provide a short title (1-6 words) and a one-line summary. Output as JSON array: {{ "title": ..., "clause": ..., "summary": ... }}\n\n{text}""")

def build_prompt_classify(text: str) -> str:
    sample = text[:2000]
    return textwrap.dedent(f"""Classify the document into one of: NDA, Lease, Employment Contract, Service Agreement, Purchase Agreement, Other. Output JSON: {{ "label": "...", "score": 0.0, "notes": "one-line reason" }}\n\n{text[:2000]}""")

# -----------------------------
# High-level task runner
# -----------------------------
def run_tasks(mode: str, hf_token: str, uploaded_file, pasted_text, task: str, clause_index: int = -1):
    # Get text
    try:
        doc_text = ""
        if uploaded_file:
            doc_text = extract_text(uploaded_file)
        if (not doc_text.strip()) and pasted_text:
            doc_text = pasted_text
        if not doc_text.strip():
            return "No text found. Upload a PDF/DOCX/TXT or paste the text."

    except Exception as e:
        return f"Error reading file: {e}"

    # If user chooses API mode in the future we could implement HF Inference API; for now only local model supported here
    if mode == "local":
        try:
            if task == "simplify_doc":
                prompt = build_prompt_simplify(doc_text)
                out = generate_with_model(prompt, hf_token)
                return out
            elif task == "simplify_clause":
                clauses = split_clauses(doc_text)
                if not clauses:
                    prompt = build_prompt_simplify(doc_text)
                else:
                    idx = clause_index if (0 <= clause_index < len(clauses)) else 0
                    prompt = build_prompt_simplify(clauses[idx])
                return generate_with_model(prompt, hf_token)
            elif task == "ner_doc":
                prompt = build_prompt_ner(doc_text)
                return generate_with_model(prompt, hf_token)
            elif task == "ner_clause":
                clauses = split_clauses(doc_text)
                if not clauses:
                    prompt = build_prompt_ner(doc_text)
                else:
                    idx = clause_index if (0 <= clause_index < len(clauses)) else 0
                    prompt = build_prompt_ner(clauses[idx])
                return generate_with_model(prompt, hf_token)
            elif task == "extract_clauses":
                prompt = build_prompt_clause_breakdown(doc_text[:15000])
                return generate_with_model(prompt, hf_token)
            elif task == "classify":
                prompt = build_prompt_classify(doc_text)
                return generate_with_model(prompt, hf_token)
            elif task == "list_clauses":
                clauses = split_clauses(doc_text)
                out = {"num_clauses": len(clauses), "clauses_preview": clauses[:20]}
                return json.dumps(out, indent=2)
            else:
                return "Unknown task."
        except Exception as e:
            return f"Inference error: {e}\n\nTip: If the model repo requires auth, provide your Hugging Face token in the HF Token box and re-run."
    else:
        return "Only local mode is implemented in this script. Set mode to 'local'."

# -----------------------------
# Gradio UI
# -----------------------------
with gr.Blocks(title="ClauseWise ‚Äî AI Legal Document Analyzer (Fixed)") as demo:
    gr.Markdown("# ClauseWise ‚Äî AI Legal Document Analyzer (Fixed)\nUpload a document (PDF/DOCX/TXT) or paste text.\nModel: ibm-granite/granite-3.3-2b-instruct (local). If the model repo requires authentication, paste HF token below.")
    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(label="Upload Document (PDF, DOCX, TXT)")
            paste_text = gr.Textbox(lines=12, label="Or paste document text here (optional)")
            hf_token = gr.Textbox(label="Hugging Face token (optional, for private/model auth)", type="password")
            mode = gr.Radio(["local"], value="local", label="Mode")
            clause_index = gr.Slider(minimum=-1, maximum=49, step=1, value=-1, label="Clause index (for clause-level tasks) (-1 = auto first)")
        with gr.Column(scale=1):
            task = gr.Dropdown(choices=[
                ("Document ‚Äî Simplify (whole document)", "simplify_doc"),
                ("Clause ‚Äî Simplify (single clause)", "simplify_clause"),
                ("Document ‚Äî NER (extract entities)", "ner_doc"),
                ("Clause ‚Äî NER (single clause)", "ner_clause"),
                ("Document ‚Äî Extract & Breakdown clauses", "extract_clauses"),
                ("Document ‚Äî Classify document type", "classify"),
                ("List detected clauses (quick heuristic)", "list_clauses")
            ], label="Task", value="extract_clauses")
            run_btn = gr.Button("Run")
            status = gr.Textbox(label="Status / Messages", interactive=False)
    output = gr.Textbox(lines=28, label="Output (model result)")

    def on_run(mode, hf_token, uploaded_file, pasted_text, task, clause_index):
        status_text = "Starting..."
        try:
            status_text = f"Running task: {task} (loading model if needed...)"
            result = run_tasks(mode, hf_token, uploaded_file, pasted_text, task, clause_index)
            status_text = "Done."
            return status_text, result
        except Exception as e:
            return f"Error: {e}", str(e)

    run_btn.click(on_run, inputs=[mode, hf_token, file_input, paste_text, task, clause_index], outputs=[status, output])

# Launch Gradio (set share=True if you need a public link)
demo.launch(share=True)

# =========================
# ClauseWise ‚Äî Single-cell Colab script (Final corrected)
# Model: ibm-granite/granite-3.3-2b-instruct (Causal LM)
# NOTE: Enable GPU in Colab for faster model loading.
# Paste this whole cell into Colab and run.
# =========================

# Install dependencies (safe even if already installed)
!pip install -q transformers accelerate huggingface-hub pdfplumber python-docx gradio requests

# ---------- Imports ----------
import os, io, re, json, textwrap, tempfile
from typing import List, Dict
import gradio as gr
import pdfplumber
import docx
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login as hf_login
import requests

# -----------------------------
# Config
# -----------------------------
MODEL_NAME = "ibm-granite/granite-3.3-2b-instruct"
MAX_NEW_TOKENS = 512
CHUNK_MAX_TOKENS = 1800  # conservative input chunking
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# -----------------------------
# Robust file reading helpers
# -----------------------------
def _read_bytes_from_maybe_fileobj(obj) -> bytes:
    if isinstance(obj, (bytes, bytearray)):
        return bytes(obj)
    if isinstance(obj, str) and os.path.exists(obj):
        with open(obj, "rb") as f:
            return f.read()
    if isinstance(obj, dict):
        if "data" in obj and isinstance(obj["data"], (bytes, bytearray)):
            return bytes(obj["data"])
        if "tmp_path" in obj and os.path.exists(obj["tmp_path"]):
            with open(obj["tmp_path"], "rb") as f:
                return f.read()
        if "name" in obj and "file" in obj and hasattr(obj["file"], "read"):
            return obj["file"].read()
    if hasattr(obj, "read") and callable(obj.read):
        data = obj.read()
        if isinstance(data, str):
            return data.encode("utf-8")
        return data
    if hasattr(obj, "name") and isinstance(obj.name, str) and os.path.exists(obj.name):
        with open(obj.name, "rb") as f:
            return f.read()
    if isinstance(obj, (list, tuple)):
        for item in obj:
            if isinstance(item, str) and os.path.exists(item):
                with open(item, "rb") as f:
                    return f.read()
    raise ValueError("Unsupported uploaded_file type ‚Äî could not read bytes. Got type: " + str(type(obj)))

def extract_text_from_pdf_bytes(b: bytes) -> str:
    out = []
    with pdfplumber.open(io.BytesIO(b)) as pdf:
        for p in pdf.pages:
            t = p.extract_text()
            if t:
                out.append(t)
    return "\n\n".join(out).strip()

def extract_text_from_docx_bytes(b: bytes) -> str:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
        tmp.write(b)
        tmp_path = tmp.name
    try:
        doc = docx.Document(tmp_path)
        paras = [p.text for p in doc.paragraphs if p.text and p.text.strip()]
        return "\n\n".join(paras).strip()
    finally:
        try:
            os.remove(tmp_path)
        except Exception:
            pass

def extract_text_from_txt_bytes(b: bytes) -> str:
    try:
        return b.decode("utf-8")
    except Exception:
        return b.decode("latin-1", errors="ignore")

def extract_text(uploaded_file) -> str:
    if uploaded_file is None:
        return ""
    raw_bytes = _read_bytes_from_maybe_fileobj(uploaded_file)
    file_name = None
    if isinstance(uploaded_file, dict) and "name" in uploaded_file:
        file_name = uploaded_file["name"].lower()
    elif hasattr(uploaded_file, "name"):
        try:
            file_name = uploaded_file.name.lower()
        except Exception:
            file_name = None
    if file_name and file_name.endswith(".pdf"):
        return extract_text_from_pdf_bytes(raw_bytes)
    if file_name and (file_name.endswith(".docx") or file_name.endswith(".doc")):
        return extract_text_from_docx_bytes(raw_bytes)
    if file_name and file_name.endswith(".txt"):
        return extract_text_from_txt_bytes(raw_bytes)
    header = raw_bytes[:8]
    if header.startswith(b"%PDF"):
        return extract_text_from_pdf_bytes(raw_bytes)
    if header.startswith(b"PK\x03\x04"):
        return extract_text_from_docx_bytes(raw_bytes)
    return extract_text_from_txt_bytes(raw_bytes)

# -----------------------------
# Clause splitting helper
# -----------------------------
def split_clauses(text: str, min_len=40) -> List[str]:
    if not text:
        return []
    parts = re.split(r'\n{2,}|\r\n{2,}', text)
    clauses = []
    for p in parts:
        sub = re.split(r'(?m)^\s*\d+\.\s+|\n-\s+|;\s+', p)
        for s in sub:
            s2 = s.strip()
            if len(s2) >= min_len:
                clauses.append(s2)
    if not clauses:
        sentences = re.split(r'(?<=[.!?])\s+', text)
        clauses = [s.strip() for s in sentences if len(s.strip()) >= min_len]
    seen = set()
    out = []
    for c in clauses:
        if c not in seen:
            seen.add(c)
            out.append(c)
    return out

# -----------------------------
# Prompt templates
# -----------------------------
NER_PROMPT = textwrap.dedent("""\
Extract all legal entities from the TEXT exactly and return ONLY a JSON array.
Entity types to identify: PARTY, DATE, TERM_DURATION, GOVERNING_LAW, JURISDICTION, AMOUNT, OBLIGATION, OTHER.
For each entity return: { "type": "...", "text": "...", "span_start": number, "span_end": number }
Rules:
- span_start and span_end MUST be EXACT character indices matching the original TEXT provided.
- Use type JURISDICTION for courts/venues (e.g., "Hyderabad courts").
- Use GOVERNING_LAW for law references (e.g., "laws of India").
- Return pure valid JSON array only (no extra commentary).
TEXT:
""")

SIMPLIFY_PROMPT = "Simplify the following legal clause/document into plain English while preserving legal meaning. Output should be concise and start with 'Simplified:'.\n\n"
CLAUSE_BREAKDOWN_PROMPT = "Break the following document into numbered clauses. For each clause give a title (1-6 words) and a one-line summary. Return JSON array objects: {\"title\":..., \"clause\":..., \"summary\":...}\n\n"
CLASSIFY_PROMPT = "Classify the document into one of: NDA, Lease, Employment Contract, Service Agreement, Purchase Agreement, Other. Return JSON object: {\"label\":\"...\",\"score\":number,\"notes\":\"one-line reason\"}\n\n"

# -----------------------------
# Model loader (local) using causal LM & trust_remote_code
# -----------------------------
_tokenizer = None
_model = None
_model_device = DEVICE

def load_model(hf_token: str = None):
    global _tokenizer, _model, _model_device
    if _model is not None and _tokenizer is not None:
        return _tokenizer, _model
    # Optional login to HF (if model/repo requires auth)
    if hf_token:
        try:
            hf_login(hf_token)
        except Exception as e:
            print("HF login warning:", e)
    try:
        print(f"Loading tokenizer & model {MODEL_NAME} (trust_remote_code=True) on {DEVICE} ...")
        _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
        _model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True,
            device_map="auto" if DEVICE == "cuda" else None,
            torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
            low_cpu_mem_usage=True
        )
        # ensure tokenizer has pad token
        if _tokenizer.pad_token is None:
            _tokenizer.pad_token = _tokenizer.eos_token
        print("Model loaded.")
    except Exception as e:
        raise RuntimeError(
            "Failed to load model locally. If the repository requires authentication, paste your HF token in the UI. "
            "Full error: " + str(e)
        )
    return _tokenizer, _model

# -----------------------------
# Safe generation helper (handles chunking/truncation)
# -----------------------------
def generate_text(prompt: str, hf_token: str = None, max_new_tokens: int = MAX_NEW_TOKENS) -> str:
    tok, mdl = load_model(hf_token)
    device = next(mdl.parameters()).device
    # Tokenize prefix length; if prompt is too long we must truncate to fit model context
    inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=CHUNK_MAX_TOKENS).to(device)
    gen_ids = mdl.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, eos_token_id=tok.eos_token_id)
    out = tok.decode(gen_ids[0], skip_special_tokens=True)
    # If out contains prompt echoed, strip prefix
    if out.startswith(prompt):
        out = out[len(prompt):].strip()
    return out

# -----------------------------
# JSON extraction utilities and span computation
# -----------------------------
def extract_json_from_model_output(text: str):
    """
    Find the first JSON array/block in the model output and parse it.
    Returns Python object or raises ValueError.
    """
    # find first '[' and last matching ']' pair (naive but practical)
    start = text.find("[")
    end = text.rfind("]")
    if start == -1 or end == -1 or end < start:
        # Could be an object instead of array
        start_obj = text.find("{")
        end_obj = text.rfind("}")
        if start_obj == -1 or end_obj == -1 or end_obj < start_obj:
            raise ValueError("No JSON array/object found in model output.")
        json_text = text[start_obj:end_obj+1]
    else:
        json_text = text[start:end+1]
    # Try JSON loads
    try:
        return json.loads(json_text)
    except Exception as e:
        # If parsing fails, try to clean common issues (single quotes)
        try:
            cleaned = json_text.replace("'", '"')
            return json.loads(cleaned)
        except Exception:
            raise ValueError(f"Failed to parse JSON from model output. Error: {e}\nOutput snippet:\n{json_text[:1000]}")

def compute_spans(text: str, entities: List[Dict]) -> List[Dict]:
    """
    For each entity dict that has 'text', compute span_start and span_end using first exact occurrence.
    If exact text not found, set -1, -1.
    """
    for ent in entities:
        ent_text = ent.get("text", "")
        if not ent_text:
            ent["span_start"] = -1
            ent["span_end"] = -1
            continue
        # find first occurrence (exact)
        idx = text.find(ent_text)
        if idx >= 0:
            ent["span_start"] = idx
            ent["span_end"] = idx + len(ent_text)
        else:
            # fallback: try case-insensitive search
            idx2 = text.lower().find(ent_text.lower())
            if idx2 >= 0:
                ent["span_start"] = idx2
                ent["span_end"] = idx2 + len(ent_text)
            else:
                ent["span_start"] = -1
                ent["span_end"] = -1
    return entities

# -----------------------------
# Task functions
# -----------------------------
def do_ner(text: str, hf_token: str = None) -> str:
    prompt = NER_PROMPT + text
    out = generate_text(prompt, hf_token)
    try:
        entities = extract_json_from_model_output(out)
        if not isinstance(entities, list):
            raise ValueError("NER model did not return a JSON array.")
    except Exception as e:
        return f"NER output parsing error: {e}\n\nRaw model output:\n{out}"
    # compute spans accurately
    entities = compute_spans(text, entities)
    return json.dumps(entities, indent=2)

def do_simplify(text: str, hf_token: str = None) -> str:
    prompt = SIMPLIFY_PROMPT + text
    out = generate_text(prompt, hf_token)
    return out

def do_clause_breakdown(text: str, hf_token: str = None) -> str:
    prompt = CLAUSE_BREAKDOWN_PROMPT + text
    out = generate_text(prompt, hf_token)
    try:
        parsed = extract_json_from_model_output(out)
        return json.dumps(parsed, indent=2)
    except Exception as e:
        # fallback: return raw output if JSON extraction fails
        return f"Clause breakdown JSON parse warning: {e}\n\nRaw output:\n{out}"

def do_classify(text: str, hf_token: str = None) -> str:
    prompt = CLASSIFY_PROMPT + text[:2000]
    out = generate_text(prompt, hf_token)
    try:
        parsed = extract_json_from_model_output(out)
        return json.dumps(parsed, indent=2)
    except Exception as e:
        return f"Classification JSON parse warning: {e}\n\nRaw output:\n{out}"

def list_clauses_quick(text: str) -> str:
    clauses = split_clauses(text)
    out = {"num_clauses": len(clauses), "clauses_preview": clauses[:20]}
    return json.dumps(out, indent=2)

# -----------------------------
# Orchestration + Gradio UI
# -----------------------------
def run_pipeline(mode: str, hf_token: str, uploaded_file, pasted_text: str, task: str, clause_index: int):
    # Acquire text
    try:
        doc_text = ""
        if uploaded_file:
            doc_text = extract_text(uploaded_file)
        if (not doc_text.strip()) and pasted_text and pasted_text.strip():
            doc_text = pasted_text
        if not doc_text.strip():
            return "No text found. Upload a PDF/DOCX/TXT or paste the text.", ""
    except Exception as e:
        return f"Error reading file: {e}", ""

    # Prepare task
    try:
        if task == "simplify_doc":
            out = do_simplify(doc_text, hf_token)
        elif task == "simplify_clause":
            clauses = split_clauses(doc_text)
            if not clauses:
                out = do_simplify(doc_text, hf_token)
            else:
                idx = clause_index if (0 <= clause_index < len(clauses)) else 0
                out = do_simplify(clauses[idx], hf_token)
        elif task == "ner_doc":
            out = do_ner(doc_text, hf_token)
        elif task == "ner_clause":
            clauses = split_clauses(doc_text)
            if not clauses:
                out = do_ner(doc_text, hf_token)
            else:
                idx = clause_index if (0 <= clause_index < len(clauses)) else 0
                out = do_ner(clauses[idx], hf_token)
        elif task == "extract_clauses":
            out = do_clause_breakdown(doc_text, hf_token)
        elif task == "classify":
            out = do_classify(doc_text, hf_token)
        elif task == "list_clauses":
            out = list_clauses_quick(doc_text)
        else:
            out = "Unknown task."
    except Exception as e:
        out = f"Inference error: {e}\nTip: If the model repo requires authentication, paste HF token in the UI and retry."

    return "Done.", out

# Build Gradio UI
with gr.Blocks(title="ClauseWise ‚Äî Final (Granite 3.3-2b instruct)") as demo:
    gr.Markdown("# ClauseWise ‚Äî AI Legal Document Analyzer (Final)\nUpload PDF/DOCX/TXT or paste text. Model: ibm-granite/granite-3.3-2b-instruct (local)\nIf loading fails due to access restrictions, paste your Hugging Face token in the HF Token box.")
    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(label="Upload Document (PDF / DOCX / TXT)")
            paste_text = gr.Textbox(lines=12, label="Or paste document text (optional)")
            hf_token = gr.Textbox(label="Hugging Face token (optional, required for private repos)", type="password")
            mode = gr.Radio(["local"], value="local", label="Mode (local)")
            clause_index = gr.Slider(minimum=-1, maximum=49, step=1, value=-1, label="Clause index (for clause tasks) (-1 = auto first)")
        with gr.Column(scale=1):
            task = gr.Dropdown(choices=[
                ("Document ‚Äî Simplify", "simplify_doc"),
                ("Clause ‚Äî Simplify (single clause)", "simplify_clause"),
                ("Document ‚Äî NER (extract entities)", "ner_doc"),
                ("Clause ‚Äî NER (single clause)", "ner_clause"),
                ("Document ‚Äî Extract & Breakdown clauses", "extract_clauses"),
                ("Document ‚Äî Classify document type", "classify"),
                ("List detected clauses (quick heuristic)", "list_clauses")
            ], label="Task", value="extract_clauses")
            run_btn = gr.Button("Run")
            status = gr.Textbox(label="Status", interactive=False, value="")
    output = gr.Textbox(lines=28, label="Output (model result / JSON)")

    def on_run(mode, hf_token, uploaded_file, paste_text, task, clause_index):
        status_text = "Starting..."
        try:
            status_text, result = run_pipeline(mode, hf_token, uploaded_file, paste_text, task, clause_index)
            return status_text, result
        except Exception as e:
            return f"Error: {e}", str(e)

    run_btn.click(on_run, inputs=[mode, hf_token, file_input, paste_text, task, clause_index], outputs=[status, output])

# Launch (set share=True if you need external link)
demo.launch(share=True)

!pip install gradio pdfplumber transformers accelerate sentencepiece

# ---------------------------
# 1. IMPORT LIBRARIES
# ---------------------------
import gradio as gr
import pdfplumber
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ---------------------------
# 2. LOAD IBM GRANITE MODEL
# ---------------------------
model_name = "ibm-granite/granite-3.3-2b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# ---------------------------
# 3. PDF TEXT EXTRACTION
# ---------------------------
def extract_text(pdf_file):
    if pdf_file is None:
        return "No file uploaded."

    try:
        text = ""
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                text += page.extract_text() + "\n"
        return text
    except:
        return "‚ùå Error reading PDF file."


# ---------------------------
# 4. AI PROCESSING FUNCTIONS
# ---------------------------
def ask_model(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=500)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def doc_simplify(pdf_file):
    text = extract_text(pdf_file)
    prompt = f"Simplify the following legal document:\n\n{text}"
    return ask_model(prompt)


def clause_simplify(text):
    return ask_model(f"Simplify this legal clause:\n{text}")


def ner_analysis(pdf_file):
    text = extract_text(pdf_file)
    prompt = f"Extract all legal entities (PARTY, DATE, TERM, LAW) in JSON:\n{text}"
    return ask_model(prompt)


def classify_document(pdf_file):
    text = extract_text(pdf_file)
    prompt = f"Classify the document type (NDA, Lease, Contract, Employment, etc.):\n{text}"
    return ask_model(prompt)


# ---------------------------
# 5. BEAUTIFUL GRADIO UI
# ---------------------------
with gr.Blocks(theme=gr.themes.Soft()) as ui:

    gr.Markdown("""
    <h1 style="text-align:center; color:#4A90E2;">üìò ClauseWise ‚Äì Legal Document Analyzer</h1>
    <p style="text-align:center;">Upload legal documents and perform AI-powered analysis.</p>
    """)

    with gr.Tabs():

        # ---- DOCUMENT SIMPLIFY ----
        with gr.Tab("üìÑ Document Simplify"):
            file_input = gr.File(label="Upload PDF Document", file_types=[".pdf"])
            btn1 = gr.Button("Simplify Document", variant="primary")
            output1 = gr.Textbox(label="Simplified Document", lines=20)
            btn1.click(doc_simplify, inputs=file_input, outputs=output1)

        # ---- CLAUSE SIMPLIFY ----
        with gr.Tab("üìù Clause Simplify"):
            clause_in = gr.Textbox(label="Enter Clause", lines=8)
            btn2 = gr.Button("Simplify Clause")
            output2 = gr.Textbox(label="Simplified Clause", lines=12)
            btn2.click(clause_simplify, inputs=clause_in, outputs=output2)

        # ---- NER ----
        with gr.Tab("üîç Entity Extraction"):
            file_input2 = gr.File(label="Upload PDF", file_types=[".pdf"])
            btn3 = gr.Button("Extract Entities")
            output3 = gr.Textbox(label="NER JSON Output", lines=20)
            btn3.click(ner_analysis, inputs=file_input2, outputs=output3)

        # ---- CLASSIFICATION ----
        with gr.Tab("üìë Document Type Classification"):
            file_input3 = gr.File(label="Upload PDF", file_types=[".pdf"])
            btn4 = gr.Button("Classify Document")
            output4 = gr.Textbox(label="Document Type", lines=5)
            btn4.click(classify_document, inputs=file_input3, outputs=output4)

ui.launch()

import gradio as gr

CUSTOM_CSS = """
body {
    background: linear-gradient(135deg, #88c1ff, #d8b4fe, #fbc2eb);
    background-size: 400% 400%;
    animation: gradientShift 15s ease infinite;
    font-family: 'Segoe UI', sans-serif;
}

@keyframes gradientShift {
    0% {background-position: 0% 50%;}
    50% {background-position: 100% 50%;}
    100% {background-position: 0% 50%;}
}

.gradio-container {
    background: rgba(255,255,255,0.7) !important;
    padding: 20px;
    border-radius: 18px;
    backdrop-filter: blur(14px);
}

h1 {
    font-size: 38px !important;
    color: #4A00E0 !important;
    text-shadow: 0px 2px 6px rgba(0,0,0,0.2);
}

.tab-nav button {
    background: #ffffff !important;
    border-radius: 10px !important;
    font-weight: bold !important;
    font-size: 16px !important;
}

button {
    background: linear-gradient(90deg, #6a11cb, #2575fc) !important;
    color: white !important;
    font-size: 18px !important;
    border-radius: 12px !important;
    padding: 10px 22px !important;
    box-shadow: 0px 4px 10px rgba(0,0,0,0.25) !important;
    transition: 0.3s ease !important;
}

button:hover {
    transform: scale(1.05) !important;
}

textarea, input {
    border-radius: 12px !important;
    border: 2px solid #6a11cb !important;
    background: rgba(255,255,255,0.8) !important;
}
"""

with gr.Blocks(css=CUSTOM_CSS, theme=gr.themes.Soft()) as ui:

    gr.Markdown("""
    <h1 style="text-align:center;">‚ú® ClauseWise ‚Äì AI Legal Document Analyzer</h1>
    <p style="text-align:center; font-size:18px;">
        Transform legal documents with powerful AI ‚Äî Simplify, Extract, Classify.
    </p>
    """)

    with gr.Tabs():

        with gr.Tab("üìÑ Document Simplify"):
            file_input = gr.File(label="Upload PDF Document", file_types=[".pdf"])
            btn1 = gr.Button("Simplify Document")
            output1 = gr.Textbox(label="Simplified Document", lines=20)
            btn1.click(doc_simplify, inputs=file_input, outputs=output1)

        with gr.Tab("üìù Clause Simplify"):
            clause_in = gr.Textbox(label="Enter Clause", lines=8)
            btn2 = gr.Button("Simplify Clause")
            output2 = gr.Textbox(label="Simplified Clause", lines=12)
            btn2.click(clause_simplify, inputs=clause_in, outputs=output2)

        with gr.Tab("üîç Extract Legal Entities"):
            file_input2 = gr.File(label="Upload PDF", file_types=[".pdf"])
            btn3 = gr.Button("Extract Entities")
            output3 = gr.Textbox(label="NER Output (JSON)", lines=20)
            btn3.click(ner_analysis, inputs=file_input2, outputs=output3)

        with gr.Tab("üìë Document Type Classification"):
            file_input3 = gr.File(label="Upload PDF", file_types=[".pdf"])
            btn4 = gr.Button("Classify Document")
            output4 = gr.Textbox(label="Document Type", lines=5)
            btn4.click(classify_document, inputs=file_input3, outputs=output4)

ui.launch()